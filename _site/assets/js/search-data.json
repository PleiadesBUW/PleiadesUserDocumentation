{"0": {
    "doc": "BeeGFS",
    "title": "BeeGFS",
    "content": "A BeeGFS cluster file system is installed on fugg1.pleiades.uni-wuppertal.de login machine and on all cluster worker nodes, i.e. the file system is shared among all nodes and can be used to develop code and to save output files from cluster jobs. In general, no „copy constructions“ are needed. A group quota according to the share of each participating group has been applied. The quota can be checked in our monitoring system or with beegfs-ctl: . beegfs-ctl --getquota --gid &lt;groupname&gt; Quota information for storage pool Default (ID: 1): user/group || size || chunk files name | id || used | hard || used | hard --------------|------||------------|------------||---------|--------- &lt;groupname&gt;| &lt;gid&gt;|| 51.50 TiB| 60.00 TiB|| 12345678|unlimited . You find your home directory at: . /beegfs/USERNAME . Heads up! There is no backup for the /beegfs file system. However, this file system is running on raid systems. if you need real local space on the worker nodes, use „/tmp“, but please clean up inside your jobs scripts, otherwise you will overload the nodes. We provide an example job script that automatically cleans up your files in /tmp in any case. Heads up! WHEP users . You have a different home directory (/common/home/USERNAME). Hence, you need to have the same files in . /common/home/USERNAME/.ssh . and . /beegfs/USERNAME/.ssh . This can easily be achieved by copying the whole directory. ",
    "url": "/filesystem/BeeGFS.html",
    
    "relUrl": "/filesystem/BeeGFS.html"
  },"1": {
    "doc": "wLCG Grid Certificates",
    "title": "wLCG Grid Certificates",
    "content": " ",
    "url": "/GridCertificates.html",
    
    "relUrl": "/GridCertificates.html"
  },"2": {
    "doc": "wLCG Grid Certificates",
    "title": "Background",
    "content": "After many many years, the German GridKa Certification Authority (CA) at KIT in Karlsruhe will cease operation at 11 June 2023 as the CA cert ends. As a successor the GÉANT Europe’s leading collaboration on network and related infrastructure and services for the benefit of research and education offers the service Sectigo Certificate Manager to obtain Grid user certificates starting now. ",
    "url": "/GridCertificates.html#background",
    
    "relUrl": "/GridCertificates.html#background"
  },"3": {
    "doc": "wLCG Grid Certificates",
    "title": "Introduction",
    "content": "On order to access global Grid resources, users must hold a valid personal Grid user certificate (authentication) AND users must be member of a Virtual Organization (VO) (authorization). A valid Grid user certificate is a prerequisite to request membership in a VO. Users usually have one Grid user certificate. Multiple VO membership is possible.1 . A Grid user certificate (format X509) consists of a private key with a private password and a certified public key. The private key and the password is exclusively possessed by the user and is NOT known to the Registration Authority (RA) or Certification Authority (CA) at any stage. A certificate is valid for one (1) year and can be renewed. Users get notified by the CA via email three (3) weeks before the expiration date. It is strongly recommended to renew the certificate before its expiration. The certificate can/should be copied to all devices/browsers which need it. ",
    "url": "/GridCertificates.html#introduction",
    
    "relUrl": "/GridCertificates.html#introduction"
  },"4": {
    "doc": "wLCG Grid Certificates",
    "title": "Authorities",
    "content": "The GÉANT CA is part of The International Grid Trust Federation (IGTF) hence Grid user certificates are accepted by all Grid sites in WLCG. In order to facilitate the request procedure, many institutions in Germany operated Registration Authorities (RA) which take over the necessary paper-work on behalf of the CA. BUW employees may use their ZIM account to authenticate against GÉANT and request a Grid user certificate. Use the portal Certificate Manager SSO Check to test your account. Non-BUW users can not use the portal and should rather check with their home institution officials how to proceed. ",
    "url": "/GridCertificates.html#authorities",
    
    "relUrl": "/GridCertificates.html#authorities"
  },"5": {
    "doc": "wLCG Grid Certificates",
    "title": "Procedure",
    "content": "There seem to be problems with the GEANT User Certificates when using those with for example AMI. Please use the alternative way (see 2. “In case of problems”) for the time being . | Test your credentials (account_name / password) in Certificate Manager SSO Check . If you encounter problems during the certificate request, please screenshot your status information in the SSO check portal for further debugging | Request a Grid user certificate in GEANT User Cert . | profile: GÉANT IGTF-MICS Personal | choose Enrollment Method: Key Generation | Key Type: RSA-4096 | more detailed information is available on the DFN FAQ page | In case of problems, e.g. if you don’t have an employee ZIM account and start the request as a student: Request a “Nutzerzertifikat”/”User certificate” at pki.pca.dfn.de/ | . | after a short while the new cert can be downloaded from the page | on Linux machines with Grid setups, the certificate and key files are usually placed in the directory ~/.globus/ . | Download the certs.p12 file the User Cert Manager offers you. | copy it to ~/.globus/certs.p12 | Extract a certificate from it with openssl pkcs12 -clcerts -nokeys -in certs.p12 -out usercert.pem . | the certificate is your passport, you “show” to services to authenticate yourself | . | Extract a key from it with openssl pkcs12 -nocerts -in certs.p12 -out userkey.pem. | Make sure the key can only be read by yourself: chmod 400 userkey.pem. | the key file is your secret key, that unlocks your certificate. Protect the key with a good password, do not share the key with anyone and backup the key, as it cannot be recovered, if the file got lost or you forgot the password | . | in addition import the certs.p12 into your browser: . | Firefox: Settings → Certificates → View Certificates → Your Certificates → Import | Chrome: Settings → Security → Manage Certificates → Import (depends on your operating system, that’s why we stongly recommend Firefox!) | . | . | with your user certificate as “passport” you have to register at your experiment/VO - so that your experiment/VO accepts your certificate and you can use experiment resources. | if you have already registered a (previous) certificate, you can add another certificate DN (DN= text string in your certificate, that identifies you) to your experiment account | for ATLAS, this VOMS server is the central point for your registration . | more information about Grid Certificate and VO Membership from an ATLAS point of view is available here | . | . | Depending on your browser version, it might be necessary to check in your browser’s certificate trust settings → the The USERTRUST Network certificate authority needs to be trusted for all operations . | Firefox: . | Settings → Certificates → View Certificates → Authorities in the The USERTRUST Network block, select Edit Trust if for GEANT eScience Personal CA and ensure, that all trust settings are enabled | . | Chrome: . | Settings → Security → Manage Certificates → Authorities search for org-The USERTRUST Network and ensure, that for both entries under ⋮ → Edit all trust settings are selected | . | . | to avoid problems with previous certificates, restart your browser after importing and backing up the certificate and key has been done. I.e., to quit Firefox or Chrome explicitly select Quit or Exit, respectively, from the browsers’ menues. | if everything works with your new certificate, you can optionally delete your previous certificate | . | . ",
    "url": "/GridCertificates.html#procedure",
    
    "relUrl": "/GridCertificates.html#procedure"
  },"6": {
    "doc": "wLCG Grid Certificates",
    "title": "Technicalities",
    "content": "Technically a new private/public key pair is created with every renewal. Finding certificates in Firefox browser . Preferences -&gt; Privacy &amp; Security -&gt; Certificates -&gt; View Certificates -&gt; Your Certificates (-&gt; Backup) . Extracting the cert Files . Download/export the file either form the browser or directly to the ~/.globus/usercert.p12 directory and make sure to safe the old files. Then use openssl to extract ~/.globus/usercert.pem and ~/.globus/userkey.pem. Have your export passphrase at hand! . cd ~/.globus &gt; mv certs.p12 certs.p12.old &gt; mv usercert.pem usercert.pem.old &gt; mv userkey.pem userkey.pem.old &gt; ls -l -r-------- 1 account group 8213 24. Jan 14:36 certs.p12 -r-------- 1 account group 2611 31. Jan 13:40 certs.p12.old &gt; openssl pkcs12 -clcerts -nokeys -in certs.p12 -out usercert.pem &gt; openssl pkcs12 -nocerts -in certs.p12 -out userkey.pem &gt; ls -l -r-------- 1 account group 8213 24. Jan 14:36 certs.p12 -r-------- 1 account group 8213 24. Jan 14:38 usercert.pem -r-------- 1 account group 2611 31. Jan 13:42 userkey.pem . Inspecting Grid user certificates . Please make sure your public (usercert.pem) and private (userkey.pem) keys are: . | in the correct directory, | have the correct permissions, | show your DN, | are valid, | match each other (have the same md5sum), | you remember the password. | . &gt; cd ~/.globus &gt; ls -l ... -r--r--r-- 1 account group 1728 8. Apr 09:36 usercert.pem -r-------- 1 account group 2012 8. Apr 09:36 userkey.pem &gt; openssl x509 -subject -issuer -dates -noout -in usercert.pem subject= /DC=org/DC=terena/DC=tcs/C=DE/O=Bergische Universitaet Wuppertal/CN=Harenberg, Torsten harenber@uni-wuppertal.de issuer= /C=NL/O=GEANT Vereniging/CN=GEANT eScience Personal CA 4 notBefore=Apr 20 00:00:00 2022 GMT notAfter=Apr 20 23:59:59 2023 GMT &gt; openssl x509 -noout -modulus -in usercert.pem | openssl md5 &gt; openssl rsa -noout -modulus -in userkey.pem | openssl md5 . | A Grid user certificate can be seen as an analogy to a passport, whereas the VO membership compares to a visa. &#8617; . | . ",
    "url": "/GridCertificates.html#technicalities",
    
    "relUrl": "/GridCertificates.html#technicalities"
  },"7": {
    "doc": "PLEIADES Acknowledgement",
    "title": "PLEIADES Acknowledgement",
    "content": "Our suggested wording for a PLEIADES acknowledgement in your paper is: . The computations were carried out on the PLEIADES cluster at the University of Wuppertal, which was supported by the Deutsche Forschungsgemeinschaft (DFG, grant No. INST 218/78-1 FUGG) and the Bundesministerium für Bildung und Forschung (BMBF). If you want to add a link, you can also point to https://pleiades.uni-wuppertal.de. ",
    "url": "/PleiadesAcknowledgement.html",
    
    "relUrl": "/PleiadesAcknowledgement.html"
  },"8": {
    "doc": "Access",
    "title": "Getting Started: Access",
    "content": "Getting an account . If you belong to one of the groups participating in PLEIADES, you can get an account by filling out this form. If your group was not involved in PLEIADES, please ask your group leader to contact the support before submitting an account request or consult our HPC.NRW Quick Reference Card. Questions/Support . In case of questions and problems, please use the following email address: . pleiades{at}uni-wuppertal.de . First Login and password change . You will receive your initial password from the administrators after your group leader has countersigned the user application. Please change your initial password on any PLEIADES login machine by using the . $ passwd . command. SSH Login . We recommend to create a password protected ssh-key pair to authenticate on login. Additionally you can define in your local ~/.ssh/config: . Host fugg1 User &lt;USERNAME&gt; Hostname fugg1.pleiades.uni-wuppertal.de IdentityFile ~/.ssh/&lt;KEYNAME&gt; . This way, simply using ssh fugg1 will log in correctly to the cluster. More info about ssh keys is available in the corresponding github documentation. Note: This approach is also more secure, since mis-typing the URL for uni-wuppertal.de could expose your credentials to a malicious server that is not in our control. Login (all users except “whep” users) . There are two login machine from which the cluster can be operated. They are: . fugg1.pleiades.uni-wuppertal.de fugg2.pleiades.uni-wuppertal.de . This node can be used to develop and test code. Once this is finished jobs can be submitted to the PLEIADES cluster. This machine runs CentOS 7. You can login on it using your username, which will be provided by us. Due to massive attacks from all over the world, SSH access is limited to IPs from inside the university’s network (132.195.0.0/16). In addition, a protection system is used that blocks IP numbers which have been used with several unsuccessful logins. So if you mistype your credentials too often, you will be locked out for a while. A good practice for using ssh regularly is to setup ssh-keys on your local machine and use . # On your local computer (assuming Linux): # ---------------------------------------- # if you don't have a key, e.g. ~/.ssh/id_ed25519.pub, create a new one # and set a password! ssh-keygen -t ed25519 # copy the ssh key to fugg1 ssh-copy-id USERNAME@fugg1.pleiades.uni-wuppertal.de # Login will now use the ssh key ssh USERNAME@fugg1.pleiades.uni-wuppertal.de . (from your local machine) to enable a key-based login on the frontend. Login (whep users) . The login mechanism for whep users is the same as for all other users, except for the login nodes. There are 2 login nodes running CentOS 7 (recommended) . higgs.pleiades.uni-wuppertal.de top.pleiades.uni-wuppertal.de . Only whep users can log into higgs and top!!! . ",
    "url": "/gettingstarted/access.html#getting-started-access",
    
    "relUrl": "/gettingstarted/access.html#getting-started-access"
  },"9": {
    "doc": "Access",
    "title": "Access",
    "content": " ",
    "url": "/gettingstarted/access.html",
    
    "relUrl": "/gettingstarted/access.html"
  },"10": {
    "doc": "Available Software",
    "title": "Software: Available Software",
    "content": "We offer many software installations through modules. Alternatively, you can source an LCG release, as described on the module page. Finally, you can use Singularity to set up a specific software environment by running your job in a container. List of Special Software . | CUDA . | module load 2021a CUDA/11.4.2 | . | NVHPC . | module load 2021a NVHPC/21.7 | . | TotalView: Debugger and analyzer . | module load 2021a TotalView/2021.3.9 | . | ARMForge: Debugger and analyzer . | module load 2021a ARMForge/21.1.1 | . | NAG Library through modules NAG and NAGfor . | module load 2021a intel-compilers/2021.2.0 NAG/27.3.0 | module load 2021a NAGfor/7.1.01 | module load 2022a intel-compilers/2021.4.0 NAG/29.0.0 | module load 2022a NAGfor/7.1.14 | When using NAG, you have to source ${EBROOTNAG}/nll6i29dbl/scripts/nagvars.sh with the correct arguments for your situation, e.g. “int64 vendor static” | . | Intel parallel studio XE 2020 . | Contains compilers, MPI, libraries and profiling tools like VTune Amplifier, Advisor, etc. | Parallel studio is superseded by oneAPI modules Example modules: intel-compilers, impi, imkl, VTune | If you want to set up the old version, go with source /beegfs/Tools/intel/setup.sh | . | PGI . | Collection of special purpose compilers for heterogeneous environments (CPUs &amp; GPUs) | PGI is superseded by the NVHPC module, but both approaches should work. | . | COMSOL 6, limited to certain groups | . ",
    "url": "/software/avail.html#software-available-software",
    
    "relUrl": "/software/avail.html#software-available-software"
  },"11": {
    "doc": "Available Software",
    "title": "Available Software",
    "content": " ",
    "url": "/software/avail.html",
    
    "relUrl": "/software/avail.html"
  },"12": {
    "doc": "Basics",
    "title": "Slurm: Basics",
    "content": "Submitting jobs . There are multiple login nodes available to submit jobs to the worker nodes: higgs, top, fugg1 and fugg2. Jobs can run in one out of three partitions, namely: . | normal (default), with a time limit of 3 days | short, intended for development and tests, with a time limit of 1 hour | long, with a default time limit of 7 days. Only 30 nodes at a time are allowed to execute in this partition, since this is intended for exceptional cases where jobs cannot be shortened below 3 days or composited with job dependencies where a subsequent job continues the operation. | gpu, with a time limit of 3 days. See Using GPUs for more information on how to submit jobs with GPU resources | . Think of partitions as a set of worker nodes, which are available to execute your jobs. Our beegfs (mounted in /beegfs/) serves as a shared resource between the login nodes and the worker nodes. Here is a small example job, printing the hostnames of multiple worker nodes: . $ cd /beegfs/${USER} $ cat testjob.sh #!/bin/sh #SBATCH --job-name=testjob #SBATCH --partition=normal #SBATCH --time=0-0:5:0 # days-hours:minutes:seconds #SBATCH --nodes=4-10 # at least 4 nodes, up to 10 #SBATCH --mem-per-cpu=128 # in MB srun hostname | sort $ sbatch testjob.sh Submitted batch job 106867 $ cat slurm-106867.out wn21001.pleiades.uni-wuppertal.de wn21002.pleiades.uni-wuppertal.de wn21003.pleiades.uni-wuppertal.de wn21004.pleiades.uni-wuppertal.de wn21005.pleiades.uni-wuppertal.de wn21006.pleiades.uni-wuppertal.de wn21007.pleiades.uni-wuppertal.de wn21008.pleiades.uni-wuppertal.de wn21009.pleiades.uni-wuppertal.de wn21010.pleiades.uni-wuppertal.de . It is good practice to provide as much information as possible with each job submission. This way, the scheduler can do a much better job at fitting the workload on available resources. You can personally benefit from this, since your job may run early, even with a lower priority, if it fits in an unclaimed slot! For example, consider to provide: . | Number of nodes | Maximum memory or cores per task | A reasonable time limit. Otherwise the partition maximum is assumed | . Here is a small example for scheduling a MPI job, with a locally compiled OpenMPI 4.1.1 installation. $ cat job.sh #!/bin/sh /beegfs/&lt;userfolder&gt;/openmpi/install/bin/mpirun ↵ /beegfs/&lt;userfolder&gt;/openmpi/mpi_hello_world ↵ --mca btl '^openib' $ sbatch -N4 job.sh Submitted batch job 106869 $ cat slurm-106869.out Hello world from processor wn21050.pleiades.uni-wuppertal.de, ↵ rank 1 out of 4 processors Hello world from processor wn21051.pleiades.uni-wuppertal.de, ↵ rank 2 out of 4 processors Hello world from processor wn21052.pleiades.uni-wuppertal.de, ↵ rank 3 out of 4 processors Hello world from processor wn21049.pleiades.uni-wuppertal.de, ↵ rank 0 out of 4 processors $ cd /beegfs/&lt;userfolder&gt; . More information about running MPI jobs on Slurm is available at: . | MPI on PLEIADES | https://slurm.schedmd.com/mpi_guide.html#open_mpi | https://www.open-mpi.org/faq/?category=slurm | . A single Slurm job can consist of multiple steps. In this case, the salloc command is used to allocate a fixed set of resources and then run multiple “srun -r …” commands to schedule job steps on these resources: . $ cd /beegfs/ $ cat testjob_steps.sh #!/bin/sh srun -lN2 -r 2 hostname &amp; srun -lN2 hostname &amp; sleep 5 squeue -u -s wait $ salloc -N4 testjob_steps.sh salloc: Granted job allocation 106886 salloc: Waiting for resource configuration salloc: Nodes wn[21001-21004] are ready for job 1: wn21004.pleiades.uni-wuppertal.de 0: wn21003.pleiades.uni-wuppertal.de 0: wn21001.pleiades.uni-wuppertal.de 1: wn21002.pleiades.uni-wuppertal.de STEPID NAME PARTITION USER TIME NODELIST 106886.extern extern normal 0:08 wn[21001-21004] salloc: Relinquishing job allocation 106886 . For example, you could use this to execute multiple (different) operations and finally do some clean-up, e.g. to leave your /beegfs area in a consolidated state. It is also possible to build dependencies between Slurm jobs via the “sbatch –dependency=\" option (also see man sbatch). But a single job with multiple steps has less overhead than multiple jobs and is therefore preferred! . Fair Share Details . Each user is associated with a account (think: bank-account). Every group has their own account which represent their shares in the cluster. Your job priority is then determined by a fair share algorithm that considers the group-shares, past effective usage of the cluster, job-size and -age. Scheduling large jobs will decrease your personal fair share, compared to your group colleagues, and the group accounts fair share, compared to other institute groups. This way a fair usage of the resources is ensured, while still utilizing idle cycles as much as possible. The fair share factor will recover with a half-life of 7 days and all fair share factors are reset at the beginning of the month. Useful commands . | sinfo - Show current state of worker nodes | squeue - See all running and pending jobs. | scancel - Cancel a specific job | sshare - See current fair share and cluster usage | sacct - Show information about past jobs | . Singularity and Slurm . You can use Singularity in Slurm batch script by just calling the respective command within the script and prepend a srun. Note for members of the whep group: submitting from higgs/top might be setting your singularity cache within /common/home, which is not available from the worker nodes. Consider using the SINGULARITY_CACHEDIR environment variable to define a shared location. X-Forwarding in Slurm . Slurm can use X-forwarding to redirect a GUI to the login node: . user@local$ ssh -X user@fugg1.pleiades.uni-wuppertal.de user@fugg1$ srun -p short --x11 --pty /bin/bash user@wn21X$ # setup modules and start GUI program here . Keep in mind that the batch system is mostly intended for batch processing. The interactive usage is mostly useful for quick checks or debugging. ",
    "url": "/slurm/basics.html#slurm-basics",
    
    "relUrl": "/slurm/basics.html#slurm-basics"
  },"13": {
    "doc": "Basics",
    "title": "Basics",
    "content": " ",
    "url": "/slurm/basics.html",
    
    "relUrl": "/slurm/basics.html"
  },"14": {
    "doc": "Best Practices",
    "title": "Slurm: Best Practise",
    "content": " ",
    "url": "/slurm/bestpractices.html#slurm-best-practise",
    
    "relUrl": "/slurm/bestpractices.html#slurm-best-practise"
  },"15": {
    "doc": "Best Practices",
    "title": "Shellcheck",
    "content": "shellcheck.net is very useful to highlight problems with your shell scripts! You can use it to check and improve your job scripts. ",
    "url": "/slurm/bestpractices.html#shellcheck",
    
    "relUrl": "/slurm/bestpractices.html#shellcheck"
  },"16": {
    "doc": "Best Practices",
    "title": "Other Best Practise",
    "content": "Every job submission in Slurm introduces some overhead to the batch system. If you have many short jobs of the same kind, e.g. 2000 x 30 minutes, you should combine your workload in fewer submission scripts or consider using Slurms job arrays (check our example job). This way you bundle all of these jobs in a single job submission, but still can treat the items individually as job steps. Please try to estimate a maximum execution time and set your job time limits accordingly. | Specify it with -t, shorter than the partitions maximum time | This will also increase the likelihood of your job starting early! | For longer job times than three days, if your program regularly saves states/snapshots: . | Schedule a first job to the normal partition | Use sbatch -d &lt;jobid&gt; to set a job dependency to the original job, to continue processing | . | As an exception, the long partition allows for longer jobs, but there are a couple of risks involved: . | Jobs naturally have to wait longer to start, since other (long) jobs may block the partition resources | A power or cooling failure might kill your jobs prematurely, wasting all of the computation and energy up to that point | Planned maintenance has to wait longer until your jobs are finished. In case of critical security interventions we might be even forced to kill you jobs. | All issues are mitigated if your programs have some form of checkpointing to save intermediate states. In this case, use the normal partition as described above. | . | . Coming from SGE . Previously, we used SGE to submit batch jobs. Since many concepts are similar between these kind of tools, you just have to look up the new tool names in many cases. This rosetta stone, provided by SchedMD is a good starting point. ",
    "url": "/slurm/bestpractices.html#other-best-practise",
    
    "relUrl": "/slurm/bestpractices.html#other-best-practise"
  },"17": {
    "doc": "Best Practices",
    "title": "Best Practices",
    "content": " ",
    "url": "/slurm/bestpractices.html",
    
    "relUrl": "/slurm/bestpractices.html"
  },"18": {
    "doc": "Best Practices",
    "title": "File Systems: Best Practices",
    "content": "Note: . Your files on BeeGFS are not backed up! Make sure to regularly store important results and data on another device. In case of a catastrophic failure or event, we won’t be able to restore the data. ",
    "url": "/filesystem/bestpractices.html#file-systems-best-practices",
    
    "relUrl": "/filesystem/bestpractices.html#file-systems-best-practices"
  },"19": {
    "doc": "Best Practices",
    "title": "BeeGFS and /tmp",
    "content": "Many and frequent accesses to files on /beegfs can produce significant load on the metadata servers. In consequence, the responsiveness of the shared filesystem goes bad for all users. There are a couple of things you should avoid when working on /beegfs, because of this: . | Too many files in a single directory. Each directory is managed by one of the metadata servers and having very many files (e.g. 1000+) in one directory can result in unbalanced blocking operations, if the directory is used in your jobs. | Frequent lookups in a directory, e.g. through ls or implicitly through any readdir operation in your program or programming language. The lookup results in a locked operations in the metadata server. This could happen if you frequently check for file status in your job scripts. | Starting many short running processes (seconds), with the software installed on BeeGFS. Each process is creating a new data stream to read the program data, which can overwhelm the storage system. | Using /beegfs as your working directory for frequent file I/O in your job. Please consider using the local /tmp storage. Every worker node is equipped with fast 2TB SSDs for exactly this purpose. | Afterwards you can transfer your results to a permanent storage on /beegfs | If you want to store logfiles etc., consider packing everything in a .tar file, since a single large file is better to digest on a parallel filesystem than many small files | If you use /tmp in your jobs, please make sure that you clean up the directories you created. Also consider what happens to these files, if your job gets canceled or crashes. | . | . We have a job script example that automatically cleans up the /tmp directory at the end of a job. If left files in /tmp that you want to rescue or remove manually, the best approach is to book an interactive shell on the node via: . srun -p short -w wn21053 -n1 -t 60 --pty /bin/bash . ",
    "url": "/filesystem/bestpractices.html#beegfs-and-tmp",
    
    "relUrl": "/filesystem/bestpractices.html#beegfs-and-tmp"
  },"20": {
    "doc": "Best Practices",
    "title": "Best Practices",
    "content": " ",
    "url": "/filesystem/bestpractices.html",
    
    "relUrl": "/filesystem/bestpractices.html"
  },"21": {
    "doc": "Computations",
    "title": "Getting Started: Computations",
    "content": "Job Submission . Job submission is done through the Slurm batch system. Jobs are usually submitted via a job script and the sbatch command, for example: . $ cd /beegfs/${USER} $ cat testjob.sh #!/bin/sh #SBATCH --job-name=testjob #SBATCH --partition=short #SBATCH --time=0-0:5:0 # days-hours:minutes:seconds #SBATCH --nodes=4-8 # at least 4 nodes, up to 8 #SBATCH --ntasks=8 # 16 processes #SBATCH --mem-per-cpu=128 # in MB, up to 4GB per core srun hostname | sort $ sbatch testjob.sh Submitted batch job 106867 $ cat slurm-106867.out wn21001.pleiades.uni-wuppertal.de wn21002.pleiades.uni-wuppertal.de wn21003.pleiades.uni-wuppertal.de wn21004.pleiades.uni-wuppertal.de wn21005.pleiades.uni-wuppertal.de wn21006.pleiades.uni-wuppertal.de wn21007.pleiades.uni-wuppertal.de wn21008.pleiades.uni-wuppertal.de . It is also possible to start an interactive job with: . $ srun -p short -N1 -n1 --pty /bin/bash srun: job 12040877 queued and waiting for resources srun: job 12040877 has been allocated resources $ # shell now on worker node. For more information, please refer to the more detailed Slurm documentation. ",
    "url": "/gettingstarted/computations.html#getting-started-computations",
    
    "relUrl": "/gettingstarted/computations.html#getting-started-computations"
  },"22": {
    "doc": "Computations",
    "title": "Computations",
    "content": " ",
    "url": "/gettingstarted/computations.html",
    
    "relUrl": "/gettingstarted/computations.html"
  },"23": {
    "doc": "Containers",
    "title": "Software: Containers",
    "content": "Apptainer (recently Singularity), is the most common containerization technology on HPC systems. Different compiler versions using singularity . The container management program singularity is installed on the system. If you need a different compiler version in order to be able to compile your program, you can download them as an image from the docker hub by using the command . singularity pull docker://gcc:&lt;version&gt; . This will create a .sif file in your current directory. You can then use the command . singularity shell &lt;your-.sif-file&gt; . to get an interactive shell using the specified compiler version. Compile your program in the way you need to and log out of the container in the usual way. Singularity and MPI . Singularity can be used to execute mpi-processes in a container. This can be done by using . mpirun singularity exec &lt;your-.sif-file&gt; &lt;/path/to/program&gt; . Note the order of calling mpi and singularity. Singularity will connect the mpi runtime environment on the host system with the mpi processes in the container. ",
    "url": "/software/containers.html#software-containers",
    
    "relUrl": "/software/containers.html#software-containers"
  },"24": {
    "doc": "Containers",
    "title": "Containers",
    "content": " ",
    "url": "/software/containers.html",
    
    "relUrl": "/software/containers.html"
  },"25": {
    "doc": "Job arrays",
    "title": "Slurm example: Job arrays",
    "content": "Job arrays can be used to submit many (e.g. thousands) similar small/short jobs. They can reduce the load from the central Slurm controller, so prefer job arrays over many small individual jobs! . A job script could look like this: . #!/bin/sh #SBATCH ... # Other options of sbatch # Execute a single job (this script) # with 16 steps (0-15) # But only execute 4 job steps at the same time (%4) #SBATCH --array=0-15%4 # Each job array step will run with the resource requirements: #SBATCH -n16 # Each job array step requires 16 tasks #SBATCH -N1-2 # Each job array step requires 1 or 2 nodes # You can use %a to have different output files for each job step: #SBATCH -o jobarraytests_%a.out #SBATCH -e jobarraytests_%a.err # Each job array step executes the following srun hostname srun echo $SLURM_ARRAY_TASK_ID srun sleep 10 . It makes sense to use environment variables like ${SLURM_ARRAY_TASK_ID}, to decide which arguments should go to which process in the job array. More details are available at https://slurm.schedmd.com/job_array.html. ",
    "url": "/slurm/exampleArray.html#slurm-example-job-arrays",
    
    "relUrl": "/slurm/exampleArray.html#slurm-example-job-arrays"
  },"26": {
    "doc": "Job arrays",
    "title": "Job arrays",
    "content": " ",
    "url": "/slurm/exampleArray.html",
    
    "relUrl": "/slurm/exampleArray.html"
  },"27": {
    "doc": "Job dependencies",
    "title": "Slurm example: Job dependencies",
    "content": "Job dependencies are useful to only execute a certain job, if a previous job was executed. More details are available through man sbatch. Job dependency graph . In the following example, we create the following dependency structure between jobs A, B, …, F . A /|\\ / | \\ B C D | E | \\ | \\| F . A corresponding job-submission script could look like this: . #!/usr/bin/env bash # Example of how to submit multiple jobs with job dependencies. # Submit job \"A\" with the sbatch command # sbatch returns the sentence \"Submitted batch job &lt;jobid&gt;\" # Use grep to only select the number jobidA=\"$(sbatch --job-name=A sleepjob.sh | grep -o -E '[0-9]+')\" # Now we can use the filtered job id to submit dependent jobs jobidB=\"$(sbatch --job-name=B --dependency=${jobidA} sleepjob.sh | grep -o -E '[0-9]+')\" jobidC=\"$(sbatch --job-name=C --dependency=${jobidA} sleepjob.sh | grep -o -E '[0-9]+')\" jobidD=\"$(sbatch --job-name=D --dependency=${jobidA} sleepjob.sh | grep -o -E '[0-9]+')\" jobidE=\"$(sbatch --job-name=E --dependency=${jobidB} sleepjob.sh | grep -o -E '[0-9]+')\" jobidF=\"$(sbatch --job-name=F --dependency=${jobidC},${jobidE} sleepjob.sh | grep -o -E '[0-9]+')\" . Singleton job dependency . Another interesting approach is a singleton job. Here you can submit many jobs with exactly the same job name. By using --dependency=singleton, Slurm will only execute only a single jobs with said name at any given time. An example submission script could look like this: . #!/usr/bin/env bash # Use the \"singleton\" flag to submit multiple jobs # with the same name. This combination will make sure # that only a single job runs at any given time. for I in $(seq 10) do sbatch --job-name=A --dependency=singleton jobscript.sh done # Slurm will only execute a single job with name \"A\" at any given time. # The others will wait. This structure can be used to automatically continue computations after the maximum 3 days of our normal partition. ",
    "url": "/slurm/exampleDeps.html#slurm-example-job-dependencies",
    
    "relUrl": "/slurm/exampleDeps.html#slurm-example-job-dependencies"
  },"28": {
    "doc": "Job dependencies",
    "title": "Job dependencies",
    "content": " ",
    "url": "/slurm/exampleDeps.html",
    
    "relUrl": "/slurm/exampleDeps.html"
  },"29": {
    "doc": "Slurm example: Clean up tmp directory",
    "title": "Slurm example: Clean up tmp directory",
    "content": "As mentioned in our filesystem best practices, it is a good idea to use a local work directory for each job. In the following minimal example, a local directory is created for the job. We use the trap built-in command to call a clean_up function in any case. This way, the work directory is cleaned up, even if the job fails and the job script is not executed until its end. #!/usr/bin/env bash #SBATCH --nodes 1 #SBATCH --ntasks 1 # Set a specific directory with a job-unique name: workdir=\"/tmp/${USER:?}_${SLURM_JOB_ID:?}\" submitdir=\"${SLURM_SUBMIT_DIR:?}\" mkdir -p \"${workdir}\" function clean_up { # Leave ${workdir} cd \"${submitdir}\" || exit # Use :? to only remove if the variable is defined. Otherwise exit rm -rf \"${workdir:?}\" exit } # Always call \"clean_up\" when script ends # This even executes on job failure/cancellation trap 'clean_up' EXIT cd \"${workdir}\" || exit # Start real work in workdir echo \"hello world\" &gt; \"test.log\" cat test.log sleep 10 . ",
    "url": "/slurm/exampleTmp.html",
    
    "relUrl": "/slurm/exampleTmp.html"
  },"30": {
    "doc": "Frequently Answered Questions",
    "title": "Frequently Answered Questions",
    "content": "In case of questions, please contact us at pleiades@uni-wuppertal.de . Q: Can I get access to the cluster? . A: Yes, if you are part of one of the involved groups. There are also limited resources for any other member of the University of Wuppertal. You can also consult our Quick Reference Card to study the access requirements. To request an account, fill out this form. Q: How do I apply for an account? . A: Fill out the application form and send it to pleiades@uni-wuppertal.de. Q: How can I use software on the cluster? . A: There is a range of centrally provided software (also see our software documentation). Users and groups typically install their own software additionally in their HOME directories. Q: Can you install a specific software for me? . A: If the software is interesting to other users/groups and is mentioned on the list of supported software by EasyBuild, we can give it a try. We cannot promise to fulfill any installation or update request, since managing software is a very time consuming task, but we will respond to requests on a best effort basis. Q: Why is loading software modules so slow? . A: Software modules are shipped via our BeeGFS shared file system. It can happen that running computations produce a heavy load on BeeGFS, which negatively affects the performance. Q: How can I do file transfers? . A: See our filetransfer documentation. Q: Where should I compile my programs? . A: Our login nodes have different hardware than our compute nodes (CPUs, InfiniBand). This is why it is a good idea to generally compile where you compute. Q: Do I have to make sure that I don’t use too many resources? . A: Yes and no. You should always try to minimize core-hours and number of jobs in order to be efficient. But if your computations are a requirement for your research, you can submit the jobs in any case. Slurms fair-share system will automatically ensure a fair scheduling of jobs between users in a group and between all groups. Q: Why is my job not starting? . A: The squeue command has a Reason column, providing more information of why pending jobs are not starting. Maybe there are jobs with a higher priority. Some requests are impossible to schedule, because of conflicting requirements, e.g. more than 3 days on the normal partition, too many CPU cores per Node (max 64), etc.. ",
    "url": "/faq.html",
    
    "relUrl": "/faq.html"
  },"31": {
    "doc": "File Systems",
    "title": "File Systems",
    "content": " ",
    "url": "/filesystem.html",
    
    "relUrl": "/filesystem.html"
  },"32": {
    "doc": "Moving Data",
    "title": "File Systems: Moving Data",
    "content": "The login nodes, especially fugg1 and 2, are mostly intended for job submissions. If you expect to move larger amounts of data, e.g. to a local computer, consider submitting a job that moves the data from a worker node to your system. This way, you can shift the workload away from login nodes. Files are usually transferred with scp or rsync. ",
    "url": "/filesystem/filetransfers.html#file-systems-moving-data",
    
    "relUrl": "/filesystem/filetransfers.html#file-systems-moving-data"
  },"33": {
    "doc": "Moving Data",
    "title": "Moving Data",
    "content": " ",
    "url": "/filesystem/filetransfers.html",
    
    "relUrl": "/filesystem/filetransfers.html"
  },"34": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": "These pages cover everything from requesting an account, important first steps, to using available software and submitting a simple job. We also describe our monitoring system, which can be useful to get a better picture of the activity on our cluster . Links to more detailed information are provided in text. ",
    "url": "/gettingstarted.html",
    
    "relUrl": "/gettingstarted.html"
  },"35": {
    "doc": "Using GPUs",
    "title": "Slurm: Using GPUs",
    "content": "This guide is introducing our newly available GPU nodes. The configuration is still very fresh, so please contact us if there are problems or if you expect something to behave differently. Hardware . There are 5 identical GPU nodes available, called gpu21[001-005]. Each node has the following specs: . | 8 GPUs: NVidia HGX A100 | 128 Cores: Two sockets with AMD EPYC 7763 64-Core Processors each | 2TB Memory, resulting in 16GB per core | 14TB of disk space at /data/. Please clean up your data when you are done | . Certain cores are associated to a certain GPU and can check the affinity between them with nvidia-smi: . $ srun -N1 -p gpu -A &lt;yourGroupAccount&gt;_gpu -n1 --cpus-per-task 128 --gpus 8 --gpus-per-task 8 nvidia-smi topo -m GPU0 GPU1 GPU2 GPU3 GPU4 GPU5 GPU6 GPU7 mlx5_0 CPU Affinity NUMA Affinity GPU0 X NV12 NV12 NV12 NV12 NV12 NV12 NV12 SYS 48-63 3 GPU1 NV12 X NV12 NV12 NV12 NV12 NV12 NV12 SYS 48-63 3 GPU2 NV12 NV12 X NV12 NV12 NV12 NV12 NV12 PXB 16-31 1 GPU3 NV12 NV12 NV12 X NV12 NV12 NV12 NV12 PXB 16-31 1 GPU4 NV12 NV12 NV12 NV12 X NV12 NV12 NV12 SYS 112-127 7 GPU5 NV12 NV12 NV12 NV12 NV12 X NV12 NV12 SYS 112-127 7 GPU6 NV12 NV12 NV12 NV12 NV12 NV12 X NV12 SYS 80-95 5 GPU7 NV12 NV12 NV12 NV12 NV12 NV12 NV12 X SYS 80-95 5 mlx5_0 SYS SYS PXB PXB SYS SYS SYS SYS X Legend: X = Self SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI) NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU) PXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge) PIX = Connection traversing at most a single PCIe bridge NV# = Connection traversing a bonded set of # NVLinks . Job Submission and Accounting . The GPU nodes are available in their own gpu partition: . $ scontrol show partition gpu PartitionName=gpu AllowGroups=ALL AllowAccounts=astro_gpu,cobra_gpu,fugg_gpu,imacm_gpu,lrz_gpu,modellierung_gpu,ops_gpu,optimierung_gpu,risiko_gpu,stroemung_gpu,whep_gpu,zim_gpu AllowQos=ALL AllocNodes=ALL Default=NO QoS=N/A DefaultTime=NONE DisableRootJobs=NO ExclusiveUser=NO GraceTime=0 Hidden=NO MaxNodes=UNLIMITED MaxTime=3-00:00:00 MinNodes=0 LLN=NO MaxCPUsPerNode=UNLIMITED Nodes=gpu21[001-005] PriorityJobFactor=1 PriorityTier=1 RootOnly=NO ReqResv=NO OverSubscribe=NO OverTimeLimit=NONE PreemptMode=OFF State=UP TotalCPUs=640 TotalNodes=5 SelectTypeParameters=NONE JobDefaults=(null) DefMemPerCPU=16000 MaxMemPerCPU=32000 . As you can see in AllowAccounts, we only allow a special purpose Slurm account with a suffix of _gpu to submit jobs to the node. Every user is associated to their groups account and a corresponding _gpu account. This way we can have separate fairshare configurations between CPU resources and GPU resources, which might be necessary to prioritize research groups with a larger demand for GPU resources. You can check the current share distribution with sshare yourself. Slurm treats GPUs as consumable resources and you must specify how many GPUs you would like to request for your job with --gpus 8. Additionally you should decide how many tasks (e.g. processes, option -n) you plan to run on a given node and how many --gpus-per-task and --cpus-per-task you require. All of these options depend on the scaling behavior of you code and how many CPUs per GPU you need. To summarize, whenever you intend to submit a job with GPU resources, consider the following options: . | -p gpu to submit to the gpu partition | -A &lt;yourGroupAccount&gt;_gpu to use your groups special purpose account for the fairshare evaluation | Number of nodes, e.g. -N1, and number of tasks, e.g. -n1 | --gpus &lt;N&gt;, where N is the number of GPUs you require for your job (up to 8 per node) | --gpus-per-task &lt;N&gt; to set the number of GPUs to be used in a single process/task (up to 8) | --cpus-per-task &lt;N&gt; to set the number of CPU cores in a single task (up to 128 per node) | . Here is an example job script, submitting 8 processes, each with 16 cores and one GPU to a single node: . #!/bin/sh #SBATCH --job-name=gputests #SBATCH --partition=gpu #SBATCH --account=&lt;groupname&gt;_gpu #SBATCH -N 1 #SBATCH --ntasks 8 #SBATCH --cpus-per-task 16 #SBATCH --gpus-per-task 1 #SBATCH --time=0-01:00:00 #SBATCH -o %x-%j.out srun -n8 nvidia-smi topo -m . Software . Some basic packages are installed on all GPU nodes: . # yum list installed | grep nvidia Loaded plugins: fastestmirror, nvidia cuda-drivers.x86_64 495.29.05-1 @local-centos7.9-x86_64--install-repos-nvidia kmod-nvidia-latest-dkms.x86_64 3:495.29.05-1.el7 @local-centos7.9-x86_64--install-repos-nvidia nvidia-driver-latest-dkms.x86_64 3:495.29.05-1.el7 @local-centos7.9-x86_64--install-repos-nvidia nvidia-driver-latest-dkms-NVML.x86_64 3:495.29.05-1.el7 @local-centos7.9-x86_64--install-repos-nvidia nvidia-driver-latest-dkms-NvFBCOpenGL.x86_64 3:495.29.05-1.el7 @local-centos7.9-x86_64--install-repos-nvidia nvidia-driver-latest-dkms-cuda.x86_64 3:495.29.05-1.el7 @local-centos7.9-x86_64--install-repos-nvidia nvidia-driver-latest-dkms-cuda-libs.x86_64 3:495.29.05-1.el7 @local-centos7.9-x86_64--install-repos-nvidia nvidia-driver-latest-dkms-devel.x86_64 3:495.29.05-1.el7 @local-centos7.9-x86_64--install-repos-nvidia nvidia-driver-latest-dkms-libs.x86_64 3:495.29.05-1.el7 @local-centos7.9-x86_64--install-repos-nvidia nvidia-fabric-manager.x86_64 495.29.05-1 @local-centos7.9-x86_64--install-repos-nvidia nvidia-libXNVCtrl.x86_64 3:495.29.05-1.el7 @local-centos7.9-x86_64--install-repos-nvidia nvidia-libXNVCtrl-devel.x86_64 3:495.29.05-1.el7 @local-centos7.9-x86_64--install-repos-nvidia nvidia-modprobe-latest-dkms.x86_64 3:495.29.05-1.el7 @local-centos7.9-x86_64--install-repos-nvidia nvidia-persistenced-latest-dkms.x86_64 3:495.29.05-1.el7 @local-centos7.9-x86_64--install-repos-nvidia nvidia-settings.x86_64 3:495.29.05-1.el7 @local-centos7.9-x86_64--install-repos-nvidia nvidia-xconfig-latest-dkms.x86_64 3:495.29.05-1.el7 @local-centos7.9-x86_64--install-repos-nvidia yum-plugin-nvidia.noarch 0.5-1.el7 @local-centos7.9-x86_64--install-repos-nvidia . Additionally, you can try some of the available modules with GPU related features: . | CUDA/11.4.2 | NVHPC/21.7: The successor of the PGI compilers | Various debuggers and profilers, e.g. TotalView/2021.3.9 and ARMForge/21.1.1, as well as Intel parallel studio XE | TensorFlow/2.6.0 | . In all of these cases you would run module load 2021a &lt;module/version&gt; in your sbatch job script or your interactive salloc/srun allocation to gain access to the corresponding tools. The modules have not been tested in this context yet, so please get in touch if something does not work as expected. ",
    "url": "/slurm/gpu.html#slurm-using-gpus",
    
    "relUrl": "/slurm/gpu.html#slurm-using-gpus"
  },"36": {
    "doc": "Using GPUs",
    "title": "Using GPUs",
    "content": "NOTE: The GPU resources are primarily owned by the imacm and etechnik groups. All other groups are allowed to submit a limited number of jobs with a lower priority to increase utilization. We may have to block access to GPU resources by other groups in times of high demand. You can check your own group affiliation with sshare -U. ",
    "url": "/slurm/gpu.html",
    
    "relUrl": "/slurm/gpu.html"
  },"37": {
    "doc": "Hardware",
    "title": "Hardware",
    "content": "The cluster consists of 268 workernodes with 17152 Cores in total. Additionally, there are 5 GPU nodes with 8 NVidia A100 GPUs and 128 Cores each. 931 TB of parallel storage are provided with BeeGFS. Node Types . | Login: . | fugg1 and fugg2: Virtual machine on Intel Xeon Gold 6238R CPUs . | The fugg login nodes are intended for job submission and light interactive workloads. | Because of architecture differences and limited resources, you should try to compile your programs through our batch system or work in interactive slurm sessions. | fugg2 has infiniband-enabled access to BeeGFS | . | top and higgs (only available to whep users) | . | wn21[001-268]: . | 2 sockets with AMD EPYC 7452 32-Core processor. 64 Cores in total | Hyperthreading disabled | 256GB memory, 4GB per thread | . | gpu21[001-005]: . | 8 GPUs: NVidia HGX A100 | 2 sockets with AMD EPYC 7763 64-Core processor. 128 Cores in total | Hyperthreading disabled | 2TB memory, 16GB per thread | More details here: Using GPUs | . | wn19[01-08]: Intel(R) Xeon(R) Gold 6152 CPU @ 2.10GHz . | Hyperthreading enabled | 176GB memory, 2GB per thread | . | . Network . The CPU Worker nodes wn21[001-268] are connected to each other and to the BeeGFS servers via InfiniBand and ethernet. All GPU nodes (gpu21[001-005]) are in a separate InfiniBand network and access BeeGFS via ethernet. The login nodes are currently connected to all other nodes via ethernet. Network Performance for Computing . For network intensive computing tasks between multiple nodes (e.g. MPI), the InfiniBand (IB) network should be used in any case. Our InfiniBand network topology is a non-blocking fat tree, which allows for full bandwidth between all nodes: . Up to 40 nodes are connected with 100 Gbit/s (HDR100) to each leaf switch. All leaf switches have 40 channels with 200 Gbit/s (HDR) to the spine switches. As a consequence, it is not necessary to keep multi-node jobs physically close, since the bandwidth can be maintained throughout the whole network. Your multi-node jobs likely perform well with any wn21xxx involved. A MPI job using our ethernet network, on the other, could saturate parts of or the whole network. For ethernet, the topology is similar, but with only 10 Gbit/s to each node and much fewer leaf-spine connections. ",
    "url": "/hardware.html",
    
    "relUrl": "/hardware.html"
  },"38": {
    "doc": "PLEIADES User Documentation",
    "title": "PLEIADES User Documentation",
    "content": "PLEIADES User Documentation . Welcome to the user documentation of the Scientific Computing Center PLEIADES, at the University of Wuppertal. This page provides an introduction to our cluster, as well as best practices and answers to common questions. If you are a new user, please read our getting started guide, the hardware description. Note, that there are multiple best practice pages with useful tips and solutions to common problems. There is also more information about Slurm, using GPU nodes, MPI and available Software. In case of further questions you can contact us at . pleiades{at}uni-wuppertal.de . More Resources . | Official PLEIADES page | The hpc-wiki.info contains many useful explanations about HPC topics, as well as Tutorials, covering . | Linux introduction | Profiling with gprof | OpenMP porgramming | GPU programming | . | . ",
    "url": "/",
    
    "relUrl": "/"
  },"39": {
    "doc": "Creating custom kernels with IPython",
    "title": "Jupyter: Creating custom kernels with IPython",
    "content": "Virtual environments provide a way to create isolated and self-contained Python environments with their own set of packages and dependencies, making it easier to manage different projects without interference. In this section, we will explore the steps required to create a virtual environment using pip and venv or Conda and install the necessary packages for running JupyterLab with IPython kernels. These kernels can then be selected from the JupyterLab interface and will provide the associated virtual environment from within your JupyterLab session. As long as you want to have a virtual environment, which uses Python 3.10.4 (the Python version of JupyterHub), you can do all steps from within a JupyterLab session’s terminal. However, we highly suggest to follow our instructions while on one of the log in nodes or on a worker node. Usually, the latter should be preferred, as not to overload the log in nodes. Further, if no local Python installation is used for the kernel, the module load command should load a Python version with the affix -bare. This ensures, that virtual environments will be void of Python modules/packages upon creation (in particular the environment variable PYTHONPATH will remain empty). We first begin with instructions regarding IPython kernels based on Python 3.10.4. With the modular structure of PLEIADES’ software stack it is not trivial to utilize new IPython kernels for other Python versions than 3.10.4. Nevertheless, at the end of this section we provide instructions on how to generate IPython kernels for arbitray Python versions (supported by IPython) through pip and venv only. They require some additional modifications in the kernel.json file and correct Python paths. We do not provide support for such custom kernels and therefore user caution is advised. JupyterHub: Virtual environments with pip for custom IPython kernels (with Python 3.10.4) . Before you start, you need to make sure, that you have Python version 3.10.4 loaded inside your current environment. If you do not have a Python 3.10.4 of your own, you can simply load it through one of the modules provided by the Scientific Computing Center PLEIADES. You can make sure you are using the desired Python binaries through which python and the version through python --version. Once ready, do the following to customize your virtual environment: . | Generate the virtual environment: python -m venv &lt;name_of_virtual_env&gt; | Activate the virtual environment: source /pathToVirtualEnvironment/bin/activate | Install the ipykernel package into the virtual environment: python -m pip install ipykernel | Install any other required packages through pip: python -m pip install &lt;package1&gt; &lt;package2&gt; ... &lt;packageN&gt; | Use the ipykernel package to install a new IPython kernel for JupyterLab servers: python -m ipykernel install --user --name 'NameOfKernelALPHANUMERICAL' --display-name \"Name of kernel displayed in Jupyter\" | To not further mess with your kernel you should deactivate the corresponding virtual environment: deactivate | . After the activation of a virtual environment in step 2, you should see the name of the environment next to your usual prompt of the CLI, e.g. (name_of_virtual_env) -bash-4.2$. Likewise, the name of the environment should disappear after step 6. Once a kernel has been created in step 5, you should almost immediately have it available in any (even already running) JupyterLab server. More importantly, you can continue modifying your custom kernel simply by sourcing its virtual environment (step 2) and then repeating step 4 as much as you need to. The command from step 5 installs your kernel into your local directory at ~/.local/share/jupyter/kernels. Jupyter automatically checks this directory and makes these kernels available to the user through the JupyterLab interface. JupyterHub: Virtual environments with Conda for custom IPython kernels (with Python 3.10.4) . If you prefer to manage your packages and virtual environments with Conda, you can do this, aswell. To activate the basic Conda environment (base) do the following: . | Open a terminal in your JupyterLab server: File-&gt;New-&gt;Terminal | In the terminal enter conda activate. Afterwards you should see a (base) before the prompt indicating that you are in the base environment. | . You can deactivate this environment, like any conda environment, by typing conda deactivate. To disable the automatic activation of the base environment enter conda config --set auto_activate_base false. Often users are not interested in using only the base environment in order to prevent conflicts between differrent packages. In order to configure your own environment with packages and use it within any of your JupyterLab servers, follow these steps: . | Create your conda environment: conda create --name &lt;name_of_virtual_env&gt; | Activate it (from anywhere): conda activate &lt;name_of_virtual_env&gt; | Install any packages you like through conda | Install the ipykernel package in the chosen conda virtual environment: conda install ipykernel | Enable the kernel: python -m ipykernel install --user --name 'Name-of-my-venv' --display-name \"Displayed name of my venv\" | Start a new Jupyter notebook: File-&gt;New-&gt;Notebook | Select your kernel from the dropdown menu according to your option --display-name from point 5 | . If you prefer, you can also install the kernels while directly logged in to PLEIADES instead of the JupyterHub log in. JupyterHub: Virtual environments with pip for custom IPython kernels (other Python versions) . Utilizing kernels with a Python version other than 3.10.4 will result in the kernel not properly loading displayed in the JupyterLab interface as disconnected. This originates in the modules loaded for JupyterLab server start-up, which cause conflicts with other python libraries. To generate an IPython kernel for an arbitrary Python version you can initially proceed as you would for a custom IPython kernel with Python 3.10.4: . | Generate the virtual environment: python -m venv &lt;name_of_virtual_env&gt; | Activate the virtual environment: source /pathToVirtualEnvironment/bin/activate | Install the ipykernel package into the virtual environment: python -m pip install ipykernel | Install any other required packages through pip: python -m pip install &lt;package1&gt; &lt;package2&gt; ... &lt;packageN&gt; | Use the ipykernel package to install a new IPython kernel for JupyterLab servers: python -m ipykernel install --user --name 'NameOfKernelALPHANUMERICAL' --display-name \"Name of kernel displayed in Jupyter\" | To not further mess with your kernel you should deactivate the corresponding virtual environment: deactivate | . Comments on some of these steps are provided further above in the corresponding section. Afterwards, follow these steps (we will use an IPython kernel based on Python 3.9.5 as an example): . | Go to the location of your kernel (usually something like ~/.local/share/jupyter/kernels/NameOfKernelALPHANUMERICAL) | Change the argument vector (argv) in kernel.json from something like this: { \"argv\": [ \"/pathToVirtualEnvironment/bin/python\", \"-m\", \"ipykernel_launcher\", \"-f\", \"{connection_file}\" ], \"display_name\": \"Name of kernel displayed in Jupyter\", \"language\": \"python\", \"metadata\": { \"debugger\": true } } . to something like this: . { \"argv\": [ \"~/.local/share/jupyter/kernels/NameOfKernelALPHANUMERICAL/initKernel.sh\", \"-f\", \"{connection_file}\" ], \"display_name\": \"Name of kernel displayed in Jupyter\", \"language\": \"python\", \"metadata\": { \"debugger\": true } } . Note, that you only change the content of argv and nothing else in that file. The argument vector will now point to a file initKernel.sh, which will be executed at the start up of the kernel. | Create the file initKernel.sh in ~/.local/share/jupyter/kernels/NameOfKernelALPHANUMERICAL/ based on the following content: #!/usr/bin/env bash # Get rid of any modules loaded by JupyterHub / JupyterLab module purge # Load the same modules which you have loaded # for the creation of the virtual environment. # If you are using your own insallation of Python # without any modules then you have to leave # the next line empty. module load 2022a GCCcore/11.3.0 Python/3.9.5-bare # This is CRITICAL and should ALWAYS be at the end of your script exec /pathToVirtualEnvironment/bin/python -m ipykernel $@ . | . Keep in mind that the additional loading of modules in initKernel.sh might take some time. Keep an eye out on the kernel status, and once it states idle, you can start working. ",
    "url": "/jupyter/jupyter-kernels.html#jupyter-creating-custom-kernels-with-ipython",
    
    "relUrl": "/jupyter/jupyter-kernels.html#jupyter-creating-custom-kernels-with-ipython"
  },"40": {
    "doc": "Creating custom kernels with IPython",
    "title": "Creating custom kernels with IPython",
    "content": " ",
    "url": "/jupyter/jupyter-kernels.html",
    
    "relUrl": "/jupyter/jupyter-kernels.html"
  },"41": {
    "doc": "Custom Jupyter Notebooks on PLEIADES",
    "title": "Jupyter: Custom Jupyter Notebooks on PLEIADES",
    "content": "PLEIADES does provide easy access to JupyterLab through a hosted JupyterHub instance. However, this ease of use comes with limited resources available for selection. If more resources are required, it is still possible to run a custom interactive Jupyter Notebook on the worker nodes with a few workarounds. It is important to note that depending on the method chosen to access the notebook, it may expose your work and potentially give access to your files to other users. Therefore, please stick to the provided approach. It is also important to clean up after yourself and close any unneeded Jupyter sessions to prevent others from being unable to access necessary computing resources. Safely Accessing Jupyter Notebooks on PLEIADES . The safest way to initiate an interactive Jupyter Notebooks session is by using sockets for tunneling the notebook data. This process consists of the following three steps: . | Submit a Jupyter Notebook job to a worker node with the resources your job(s) will require. | Use ssh to tunnel from your local machine to the worker node from step 1, using a login node as a jump host. | Access the notebook through your preferred browser. | . We will now explain these steps in more detail, along with the necessary inputs. Step 1: Submitting a Jupyter Notebook Job . To initiate your Jupyter Notebook, you will need to submit a job that runs the following command: . srun -n1 jupyter-notebook --no-browser --no-mathjax --sock /beegfs/$USER/jupyter_wn.socket . This command starts a Jupyter session and directs it to the socket jupyter_wn.socket in your $HOME directory. This socket will later be used to ssh tunnel to the Jupyter notebook. When writing your batch script, keep in mind that the provided resource requirements should be based on the calculations you want to perform and not on the notebook itself. Once you have an idea of the resources you will need (e.g. CPUs, GPUs, memory), create a BATCH file as usual. Here is an example: . #!/bin/sh #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --time=01:00:00 #SBATCH --mem-per-cpu=128 #SBATCH --job-name=jupyter-nb #SBATCH --output=jupyter-%j.log #SBATCH --error=jupyter-%j.err # Some initial setup module purge # This module load instruction is where the Jupyter client comes from 2022a GCCcore/11.3.0 JupyterHub/3.1.1 JupyterLab/3.5.0 # Here you can load any additional modules you might need, e.g. for your simulations or for running R notebooks # module load my other modules # optional: if you have a Conda env, activate it here: #conda activate testenv # Start the notebook and send data through socket jupyter_wn.socket in $HOME srun -n1 jupyter-notebook --no-browser --no-mathjax --sock /beegfs/$USER/jupyter_wn.socket . Feel free to adjust resource requirements to your needs. Once the job has been submitted, you will need to wait until the Jupyter notebook is actually running. You can check the job’s status by entering squeue -u $USER | grep jupyter-nb, which will also provide you with the worker node on which the notebook is running (required for step 2). To confirm that all modules have been loaded and Jupyter is being forwarded to the socket, check the corresponding .err-file. When it contains a section with . http://localhost:8888/?token=longTokenString . you are ready for the next step. Step 2: Tunneling to the Worker Node . Worker nodes cannot be accessed from outside. This means, you cannot directly tunnel through socket jupyter_wn.socket from your local machine. Instead, you have to use one of the login nodes (fugg1 or fugg2) as a jump host (inserting the worker node from step 1): . ssh -L 8888:/beegfs/&lt;username&gt;/jupyter_wn.socket -J fugg1.pleiades.uni-wuppertal.de &lt;username&gt;@&lt;worker-node&gt;.pleiades.uni-wuppertal.de -N . You can change 8888 to a different local port if you need to. The port number is relevant in the next step. Leave that terminal window open to sustain the connection to the socket. You can close the connection any time by pressing CTRL-C. Step 3: Accessing the Notebook . Once the tunnel is set up, you can access the Jupyter notebook by directing your favorite web browser to the URL in the .err-file from step 1. This could, for example, be: . http://localhost:8888/?token=0ef77dda74cag9274nc550ae49c55cb1c25ded2a6a0 . Like in step 2 before, you need to adjust the 8888 according to your desired local port. Keep in mind, that the message in the .err-file will always use port 8888, so you might have to adjust it. The long string after token= is a security measure to prevent others from accessing your notebook. Using virtual environments for custom IPython kernels . It is possible to define your own IPython kernels based on virtual environments created and managed through Conda or a combination of pip and venv. We describe this in detail in our section on the custom IPython kernels. Closing remarks . | Please do not let your notebooks run while not using them, as they occupy valuable resources. | Closing the notebook does not mean the job will be closed, too. So please do not forget to cancle your respective jobs with scancel &lt;job-ID&gt; when done. | If you accidentally close the forwarding from step 2, you can simply repeat step 2 to reestablish connection. | If you get an error, that a port is already being used you can simply change the port. Further, you can close any process using a port with kill $(lsof -t -i:&lt;port-number&gt;). | . ",
    "url": "/jupyter/jupyter-nb_on_pleiades.html#jupyter-custom-jupyter-notebooks-on-pleiades",
    
    "relUrl": "/jupyter/jupyter-nb_on_pleiades.html#jupyter-custom-jupyter-notebooks-on-pleiades"
  },"42": {
    "doc": "Custom Jupyter Notebooks on PLEIADES",
    "title": "Custom Jupyter Notebooks on PLEIADES",
    "content": " ",
    "url": "/jupyter/jupyter-nb_on_pleiades.html",
    
    "relUrl": "/jupyter/jupyter-nb_on_pleiades.html"
  },"43": {
    "doc": "Jupyter",
    "title": "Jupyter on PLEIADES",
    "content": "Jupyter is a powerful tool for data science and interactive computing. Whether analyzing data, running simulations, or building models, Jupyter’s interactive environment and powerful tools make it easy to get work done. PLEIADES’ Jupyter documentation is divided into two sub-pages: JupyterHub Login and Custom JupyterLab servers. The JupyterHub Login page explains how to use the JupyterHub Login on PLEIADES to get automated JupyterLab servers batched through SLURM. This option is recommended for those who prefer a quick and easy way to start working with Jupyter on the cluster. Alternatively, Custom JupyterLab servers describes how to batch JupyterLab servers by yourself, providing full control over the SBATCH settings. This option is recommended for those who have specific requirements or want to customize their JupyterLab environment. ",
    "url": "/jupyter.html#jupyter-on-pleiades",
    
    "relUrl": "/jupyter.html#jupyter-on-pleiades"
  },"44": {
    "doc": "Jupyter",
    "title": "Jupyter",
    "content": " ",
    "url": "/jupyter.html",
    
    "relUrl": "/jupyter.html"
  },"45": {
    "doc": "JupyterHub on PLEIADES",
    "title": "Jupyter: JupyterHub on PLEIADES",
    "content": "PLEIADES provides users with easy access to a JupyterLab graphical user interface (GUI) that comes with preset resources. This is possible through a JupyterHub virtual machine that can only be reached from within the university’s network. Depending on the user’s location, a VPN may be necessary to access it. From the JupyterHub interface, users can automatically spawn JupyterLab servers and connect to them. The procedure for spawning is technically identical to batching any other job on PLEIADES. Therefore, depending on the current cluster load, it may take some time until the JupyterLab server job leaves the pending status (PD). Even after the job is already running (status R), it may take a few seconds for JupyterLab to establish a connection with JupyterHub because some modules need to be loaded from BeeGFS. JupyterHub: Available resources . Users cannot adjust computational resources individually. Instead, the Scientific Computing Center PLEIADES provides multiple profiles with different resources that users can choose from. These profiles have been designed to make it easy for users to interact with their code for the purpose of performance analysis and quick evaluation. If users require a more customized resource setup, we also provide a guide to interactive Jupyter-notebook sessions with individually set resource requirements. If you are using Python packages in your simulations, you need to make sure, that they are also available in your JuyterLab server sessions. If you were using your own Python installation, which is not Python 3.10.4, you will need to install the packages again with Python 3.10.4. The easiest way to do so is to start a JupyterLab server through JupyterHub and to open a bash terminal inside (File-&gt;New-&gt;Terminal). This ensures that the right pip installation will be used for python pip install --user &lt;PACKAGE&gt;. JupyterHub: Access and usage . To access JupyterHub, users can go to https://jupyterhub.pleiades.uni-wuppertal.de/. Any important information about the cluster, such as planned maintenance, will be displayed here as well. Users will be asked to enter their credentials, which are the same as their PLEIADES account. Once users have signed in and assuming they have no JupyterLab server instance running, they will be directed to the profile spawn menu. Otherwise, they will be directed to their running JupyterLab server. The available resources in the provided profiles are designed to give users the possibility of interactively evaluating their code and analyzing its multiprocessor capabilities. The following limitations apply to the JupyterLab servers initialized through JupyterHub: . | Runtime of JupyterLab servers is limited to 1 day. | If jobs spend more than 8 hours with the pending status PD, they will be automatically cancelled. This is to prevent unattended JupyterLab servers from spawning and occupying valuable resources. | Profiles with multiple CPUs or GPUs still have to spawn jobs on one single node. This means that a profile offering 4 CPUs will batch a job that waits until at least 4 CPUs are free on one node. Users should keep this in mind when available resources are limited. | . Once a profile has been selected, users can batch the JupyterLab server job by clicking on the “Start” button. This will automatically redirect users to the spawning screen, where they will remain until the job is running and connects to JupyterHub. Depending on the current cluster load, this can take some time. The graphic below shows a spawning screen with an open Event Log (opened by clicking on it). Here users can see, that . | A server has been requested, i.e. a job has been submitted. | The job has been pending in the queue (status PD). | The job has started and JupyterHub is now awaiting a connection to the JupyterLab server. | . Please note that the cluster job running (point 3) is usually not identical to the JupyterLab server being available. The batched job contains other instructions besides starting the actual JupyterLab server, and therefore it might take some time before a running job actually connects to JupyterHub. Once the connection to the JupyterLab server is established, the JupyterLab UI will be displayed as shown below. A comprehensive guide on how to use JupyterLab and its various features can be found in the official documentation. It is crucial to properly terminate your JupyterLab session as closing your browser window or logging out will not close the JupyterLab server. To end your session correctly, first, navigate to the Hub Control Panel by clicking on File-&gt;Hub Control Panel located in the top left corner of the interface. This will open the Hub Control Panel in a new window as shown below: . Clicking on My Server or on the logo of the BUW will bring you back to your running JupyterLab server, whereas clicking on Stop my Server will terminate the job. Depending on the cluster load, this might take some time, however, after clicking on Stop my Server you are free to close all windows. If you are unable to terminate your server for any reason, you can always cancel the job manually by entering scancel &lt;JOBID&gt; in the terminal on the cluster, just like any other job on SLURM. Using virtual environments for custom IPython kernels . It is possible to define your own IPython kernels based on virtual environments created and managed through Conda or a combination of pip and venv. We describe this in detail in our section on the custom IPython kernels. JupyterHub: FAQ . | What versions are you using? We are using JupyterHub 3.1.1 and JupyterLab 3.5.0. | I have mutliple jobs running on the cluster. How do I know which one is the JupyterLab server? All JuypterLab server jobs get the same name, spawner-jupyterhub, although only a few characters might be visible when entering squeue -u &lt;USERNAME&gt;. | I want to check my JuypterLab logfiles. Where are they located? The logfiles belonging to the JupyterLab job (not just for JupyterLab itself) are always written to the users home directory and have the following naming convention: jupyterhub_slurmspawner_&lt;JOBID&gt;.log | I would love to have more resources available for my JupyterLab session. Is this possible? Yes, you can manually configure and submit a JupyterLab server without using JupyterHub. Instructions on how to do this are provided here. | Why can’t I find my Python packages? Please see the section Available Resources to ensure you have installed the packages into the right Python directory. | . ",
    "url": "/jupyter/jupyterHub.html#jupyter-jupyterhub-on-pleiades",
    
    "relUrl": "/jupyter/jupyterHub.html#jupyter-jupyterhub-on-pleiades"
  },"46": {
    "doc": "JupyterHub on PLEIADES",
    "title": "JupyterHub on PLEIADES",
    "content": " ",
    "url": "/jupyter/jupyterHub.html",
    
    "relUrl": "/jupyter/jupyterHub.html"
  },"47": {
    "doc": "Modules",
    "title": "Software: Modules",
    "content": "We use LMod environment modules to provide software installations to our users. Please have a look at the LMod user guide. Finding Modules . After login, you can use the module command to find and load software: . $ module --help Usage: module [options] sub-command [args ...] Options: -h -? -H --help This help message . For example, if you need a recent version of GCC and CMake and want to see available versions, you can execute: . $ module spider GCC CMake ------------------------------------------------------------------------------- GCC: ------------------------------------------------------------------------------- Description: The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). Versions: GCC/4.9.3-2.25 GCC/8.3.0 GCC/9.3.0 GCC/10.2.0 GCC/10.3.0 Other possible modules matches: GCCcore ------------------------------------------------------------------------------- CMake: ------------------------------------------------------------------------------- Description: CMake, the cross-platform, open-source build system. CMake is a family of tools designed to build, test and package software. Versions: CMake/3.15.3 CMake/3.16.4 CMake/3.18.4 CMake/3.20.1 ------------------------------------------------------------------------------- To find other possible module matches execute: $ module -r spider '.*GCC.*' ------------------------------------------------------------------------------- For detailed information about a specific \"CMake\" package (including how to load the modules) use the module's full name. Note that names that have a trailing (E) are extensions provided by other modules. For example: $ module spider CMake/3.20.1 ------------------------------------------------------------------------------- . Use the specific versions of GCC and CMake, to see details on how to load these modules: . $ module spider GCC/10.3.0 CMake/3.20.1 ------------------------------------------------------------------------------- GCC: GCC/10.3.0 ------------------------------------------------------------------------------- Description: The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). You will need to load all module(s) on any one of the lines below before the \"GCC/10.3.0\" module is available to load. 2021a 2021a-norpath Help: Description =========== The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). More information ================ - Homepage: https://gcc.gnu.org/ ------------------------------------------------------------------------------- CMake: CMake/3.20.1 ------------------------------------------------------------------------------- Description: CMake, the cross-platform, open-source build system. CMake is a family of tools designed to build, test and package software. You will need to load all module(s) on any one of the lines below before the \"CMake/3.20.1\" module is available to load. 2021a GCCcore/10.3.0 2021a-norpath GCCcore/10.3.0 Help: Description =========== CMake, the cross-platform, open-source build system. CMake is a family of tools designed to build, test and package software. More information ================ - Homepage: https://www.cmake.org . To finally load the software you can call module load 2021a GCC/10.3.0 CMake/3.20.1: . $ gcc --version gcc (GCC) 10.3.0 Copyright (C) 2020 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. $ which gcc /beegfs/Tools/easybuild/stacks/rome/2021a/software/GCCcore/10.3.0/bin/gcc $ which cmake /beegfs/Tools/easybuild/stacks/rome/2021a/software/CMake/3.20.1-GCCcore-10.3.0/bin/cmake . You would use such a module load command, whenever you prepare your environment for interactive work, as well as in batch job scripts. Background Information: Hierarchical Naming Scheme . The module visibility is following a hierarchical naming scheme. It means that software only becomes visible to the module available command, after it’s dependencies are loaded. 2021a represents a “meta module”, acting as an entry point into a given version of the software stack. A new software stack is released about every 6 months and older stacks will be phased out over time. To ensure reproducibility, you can still access old software stacks by calling module use /beegfs/Tools/easybuild/old-meta-module/, but we recommend to use the most recent versions whenever possible. The hierarchical naming scheme can make it difficult to discover available software. If you look for something specific, just use module spider &lt;software&gt; to get an overview and module spider &lt;software&gt;/&lt;version&gt; to learn what dependencies have to be loaded. You can also look into /beegfs/Tools/easybuild/stacks/rome/2021a/software/ to get a general idea of the available software (as of 2021-11-18): . $ ls /beegfs/Tools/easybuild/stacks/rome/2021a/software/ Anaconda3 flatbuffers h5py libiconv numactl SciPy-bundle ATK flatbuffers-python HarfBuzz libjpeg-turbo NVHPC Score-P at-spi2-atk flex HDF5 libpciaccess OPARI2 SCOTCH at-spi2-core FlexiBLAS help2man libpng OpenBLAS SDL2 Autoconf fmt hwloc libreadline OpenFOAM SIONlib Automake fontconfig hypothesis LibTIFF OpenMPI snappy Autotools foss ICU libtool OpenSSL spdlog Bazel freeglut iimpi libunwind OTF2 SQLite binutils freetype imkl libxml2 Pango Szip Bison FriBidi impi libyaml PAPI Tcl Boost GCC intel likwid Paraver TensorFlow bzip2 GCCcore intel-compilers LLVM ParaView Theano cairo GDB intltool LMDB PCRE typing-extensions CFITSIO Gdk-Pixbuf ISL Lua PCRE2 UCX CGAL gettext JasPer lz4 PDT UnZip Clang giflib Java M4 Perl util-linux CMake git jbigkit makeinfo pixman Valgrind CubeGUI GL2PS JsonCpp Mako pkgconfig wxWidgets CubeLib glew JUBE MCFM pkg-config X11 CubeWriter GLib Keras Mesa PMIx x264 CUDA GMP LAME Meson protobuf x265 cURL gnuplot libarchive METIS protobuf-python xorg-macros DB GObject-Introspection libcerf MPC pybind11 XZ DBus gompi libdrm MPFR Python Yasm double-conversion gperf libepoxy NASM PyYAML Z3 Doxygen groff libevent ncurses Qt5 Zip Eigen GSL libfabric netCDF re2c zlib elfutils GST-plugins-base libffi Ninja ROOT zstd expat GStreamer libgd NSPR Rust FFmpeg GTK3 libGLU NSS ScaLAPACK FFTW gzip libglvnd nsync Scalasca . Background Information: Module Build . We build the software stacks with EasyBuild. All modules have been built on our wn21 nodes and therefore optimized for the AMD Rome architecture. If you are missing a software, please check if it is available in EasyBuilds list of supported software. In this case it might be trivial for us to provide it as well. Otherwise, we hope you understand that building unsupported software may require some time or might not be feasible for us. If you have made your own modules, e.g. in /beegfs/&lt;user&gt;/modules, you can add the path via module use /path/to/my/modules. Troubleshooting . Occasionally you are in a situation where you know a new module just has been made available, but the module command claims it is not. In this case you can try module --ignore-cache spider, since available module locations might be cached in your user directory. If you load many modules, you might experience an increased latency in your interactive shell commands. This is caused by a growing LD_LIBRARY_PATH environment variable, when modules from the 2021a (and any -norpath) stack are loaded. The growing LD_LIBRARY_PATH is causing frequent search operations on our /beegfs, which can be slow if many file system operations occur in parallel, e.g. through other users jobs. We are working on mitigations of this issue and as a workaround you can try to use the software from the 2021a-rpath stack instead. Here, the whole software stack is built with RPATH, i.e. lookup paths of library dependencies are compiled into all binaries and the lookup through LD_LIBRARY_PATH is not necessary anymore. There are a couple of drawbacks though: . | The 2021a-rpath stack is not necessarily suited to build your own software, e.g. by loading GCC and CMake. In this case you would have to consider using RPATH in your build as well. For simplicity, try to use the -rpath stack only if you intend to use the application, but not build further software against it. | The 2021a(-norpath) and 2021a-rpath stack might be out of sync. Please tell us if you need a certain module that is available in one, but no the other. | . ",
    "url": "/software/modules.html#software-modules",
    
    "relUrl": "/software/modules.html#software-modules"
  },"48": {
    "doc": "Modules",
    "title": "Alternative to Modules: LCG",
    "content": "The SFT project at CERN provides multiple LCG releases that bundle many useful software packages. This also contains different versions of the gcc or clang compilers. If CVMFS is available, you can set up a specific release via . source /cvmfs/sft.cern.ch/lcg/views/LCG_99/x86_64-centos7-gcc8-opt/setup.sh . CVMFS is available on all login and worker nodes of PLEIADES. If you are only interested in a single tool, e.g. gcc, you can source the corresponding setup.sh, e.g. in . /cvmfs/sft.cern.ch/lcg/releases/gcc/8.1.0/x86_64-centos7/setup.sh . Please check if the version of the operating system (here centos7) matches the architecture in that path. For more info refer to the LCG info web page or the README files in /cvmfs/sft.cern.ch/lcg/ . ",
    "url": "/software/modules.html#alternative-to-modules-lcg",
    
    "relUrl": "/software/modules.html#alternative-to-modules-lcg"
  },"49": {
    "doc": "Modules",
    "title": "Modules",
    "content": " ",
    "url": "/software/modules.html",
    
    "relUrl": "/software/modules.html"
  },"50": {
    "doc": "MPI on PLEIADES",
    "title": "Software: MPI on PLEIADES",
    "content": "There are multiple MPI environments available: . | OpenMPI4: module load 2021a GCC/10.3.0 OpenMPI/4.1.1 | Intel MPI: module load 2021a iimpi/2021a (oneAPI) or through Parallel Studio with source /beegfs/Tools/intel/setup.sh (Version from 2020) | Local package OpenMPI3 in /lib64/openmpi3 on our worker nodes | Compiling your own MPI libraries | LCG release (See last section of modules, not using InfiniBand) | . All MPI versions were tested on PLEIADES with an MPI benchmark. These tests covered the mpirun and srun approach (see below), as well as ethernet and infiniband communication. Many problems with MPI are caused by a mismatch between the applications expected MPI version/configuration and the used MPI version in your environment. If you experience problems, try a clean build and investigate MPI related options during your application build and at runtime. When compiling your own MPI, make sure to send a build job to the worker nodes and provide the --with-ucx flag (and possibly more!). Otherwise your MPI version is likely to not utilize our InfiniBand network and rely on ethernet communication instead. PMI Library . The PMI library acts as an interface between Slurm and various MPI versions. This way Slurm can manage MPI processes, if the MPI library has been compiled correctly, e.g. by using --with-pmi and --with-slurm. If you intend to compile your own MPI versions, you may have to mention the location of PMI libraries: . # find /lib64/ -name libpmi* /lib64/libpmi.so.1.0.1 /lib64/libpmi.la /lib64/libpmi.so /lib64/libpmi2.la /lib64/libpmi2.so /lib64/libpmi.so.1 /lib64/libpmi2.so.1 /lib64/libpmi2.so.1.0.0 /lib64/libpmix.la /lib64/libpmix.so /lib64/libpmix.so.2.2.32 /lib64/libpmix.so.2 . These paths may also become relevant if you use Intel MPI. Using MPI in Slurm Jobs . There are two approaches to use MPI in your Slurm batch scripts: . | srun | mpirun | . srun is by default executing srun --mpi=pmix_v3, which may require your software to be build against PMIx, available as mentioned above. Alternative PMI options can be listed with srun --mpi=list . Applications that implicitly ship MPI may need additional configuration, e.g. enabling slurm support or pointing to PMI libraries. This is a case-by-case situation where you should study the corresponding documentation of your application. Consider reading the Slurm MPI documentation. Deciding on the number of nodes, processes and cores per process can confusing sometimes. Use srun --cpu-bind=help to show available options to bind CPU resources managed by Slurm to your (MPI-)processes and have a look at the Slurm CPU Management guide. Example Job Script with OpenMPI 4 . #!/bin/bash #SBATCH -p short #SBATCH -t 60 #SBATCH -N 4 # 4 Nodes #SBATCH -n 4 # 4 processes in total module load 2021a GCC/10.3.0 OpenMPI/4.1.1 # Option one: srun --mpi=pmix_v3 /path/to/mpiapplication &lt;arguments&gt; # Or using mpirun directly: mpirun /path/to/mpiapplication &lt;arguments&gt; . It is possible to pass --mca options in these commands as well. Notes on Intel MPI . #!/bin/bash #SBATCH -p short #SBATCH -t 60 #SBATCH -N 4 # 4 Nodes #SBATCH -n 4 # 4 processes in total # Chose ONE of the following: # Use recent version of oneAPI impi module load 2021a iimpi/2021a # Alternatively use impi shipped with Parallel Studio 2020 source /beegfs/Tools/intel/setup.sh # Tell impi where to pick up the PMI library, paths above. # Maybe try libpmix.so or libpmi2.so, if you have problems export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so # Option one: srun --mpi=pmix_v3 /path/to/mpiapplication &lt;arguments&gt; # Or using mpirun directly: mpirun /path/to/mpiapplication &lt;arguments&gt; # Third option through the hydra process manager mpiexec.hydra -bootstrap slurm -n &lt;num_procs&gt; /path/to/mpiapplication &lt;arguments&gt; . Local OpenMPI 3 . If you need more information about the locally installed OpenMPI 3 version, you can look around a worker node by . # Look into local openmpi3 manually on WN $ salloc -N1 -n1 $ ssh wn21123 # replace with correct wn number of your interaction session $ yum info openmpi3 $ ls /lib64/openmpi3 $ &lt;ctrl-d to exit salloc&gt; . Alternatively, if you are just interested in file locations: . $ srun -N1 -n1 tree /lib64/openmpi3 | less . ",
    "url": "/software/mpi.html#software-mpi-on-pleiades",
    
    "relUrl": "/software/mpi.html#software-mpi-on-pleiades"
  },"51": {
    "doc": "MPI on PLEIADES",
    "title": "MPI on PLEIADES",
    "content": " ",
    "url": "/software/mpi.html",
    
    "relUrl": "/software/mpi.html"
  },"52": {
    "doc": "Job Performance",
    "title": "Slurm: Job Performance",
    "content": "The third party script reportseff has been installed on all login nodes. It can provide a quick overview of your jobs performance: . | TimeEff: How much of the requested job time has been used | CPUEff: Ratio of active CPU time to the maximum possible with requested number of cores | MemEff: How much of the requested memory was used at peak | . Parameters . You can use reportseff -u &lt;username&gt; to show all jobs from the past week. If the output is weird, use --no-color to disable the color command line parameters. Interpreting . A low TimeEff means, you can use a shorter time limit for your job, which is easier and faster to be scheduled by Slurm. A TImeEff 100% occurs, if your job hits the maximum time limit. A low CPUEff means, your job is not utilizing the requested cores very well. They could be limited by network or disk I/O. If you suspect the disk I/O, try working in a local work directory in /tmp. If the application itself is inefficient at scaling to larger numbers of cores, you could run with less cores or try to optimize the application itself. A large MemEff means, your job is utilizing the requested memory. Everything below 100% is probably fine, as long as your job runs as expected. A very large percentage above 100% could mean that your application expects more than 4GB of memory per core. In this case, submit your job with a specific --mem request or increase the number of cores without using them. If MemEff gets exceedingly large, your job might be canceled (out of memory), which can be caused by memory leaks etc. ",
    "url": "/slurm/performance.html#slurm-job-performance",
    
    "relUrl": "/slurm/performance.html#slurm-job-performance"
  },"53": {
    "doc": "Job Performance",
    "title": "Job Performance",
    "content": " ",
    "url": "/slurm/performance.html",
    
    "relUrl": "/slurm/performance.html"
  },"54": {
    "doc": "PGI",
    "title": "Software: PGI",
    "content": "PGI provides tools to develop programs in a heterogeneous environment (CPUs &amp; GPUs): . | Fortran, C++17, C compilers | CUDA Fortran | OpenMP 4.5 (multicore CPUs) | OpenACC 2.6 (multicore CPUs &amp; GPUs) | AVX-512 | PGI compiler assisted software testing | Debugging tools to investigate differences between CPU &amp; accelerators | Profiling tools | . The PGI license is made available through the HPC.NRW Kompetenznetzwerk. Setup . The PGI license is currently only available on our fugg1 node. Connect to it from within the university network via . ssh fugg1.pleiades.uni-wuppertal.de . Additionally you need to setup your user environment: . export PGI=\"/beegfs/pgi\" export PATH=\"${PGI}/linux86-64/20.4/bin\":$PATH export MANPATH=$MANPATH:\"${PGI}/linux86-64/20.4/man\" export LM_LICENSE_FILE=$LM_LICENSE_FILE:\"${PGI}/license.dat\" . This can also be done via . source /beegfs/pgi/scripts/setup.sh . Consider putting this into your bashrc, if you have to call it regularly! . Small Fortran Test . To check if the compilers work, lets create a small Fortran program hello.f90: . print *, \"Hello world\" end . Now compile and execute it via: . pgfortran -o hello hello.f90 ./hello . Additional Resources . Get an overview of possible command line options via the man pages, e.g. man pgfortran or through the -help flag. (not -h or --help!) . There is a lot of information on the PGI page as well: . | Dokumentation | User Guide | Profiler User Guide | OpenACC User Guide | Tutorials | Porting &amp; Tuning Guides (requires account and login) | . ",
    "url": "/software/pgi.html#software-pgi",
    
    "relUrl": "/software/pgi.html#software-pgi"
  },"55": {
    "doc": "PGI",
    "title": "PGI",
    "content": "PGI is superseded by the NVHPC module, but both approaches should work. ",
    "url": "/software/pgi.html",
    
    "relUrl": "/software/pgi.html"
  },"56": {
    "doc": "Slurm",
    "title": "Slurm",
    "content": "This is a small getting-started guide about submitting batch jobs with Slurm on the new PLEIADES cluster. A more general introduction to slurm is available at slurm.schedmd.com/quickstart.html. ",
    "url": "/slurm.html",
    
    "relUrl": "/slurm.html"
  },"57": {
    "doc": "Software & File Systems",
    "title": "Getting Started: Software &amp; File Systems",
    "content": "Available Software . Our centrally provided software installations are offered through modules. The LMod command module is used to make certain programs in a specific version available to your current shell. For example, to load a GCC 10.3.0: . $ module spider GCC/10.3.0 -------------------------------------------- GCC: GCC/10.3.0 -------------------------------------------- Description: The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...). You will need to load all module(s) on any one of the lines below before the \"GCC/10.3.0\" module is available to load. 2021a [...] $ module load 2021a GCC/10.3.0 $ gcc --version gcc (GCC) 10.3.0 Copyright (C) 2020 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . Notice the “2021a” module. We use a hierarchical module naming scheme, which means that modules only become available, if their dependencies are loaded. The 2021a module serves as an entry point into a software stack with a collection of many programs in a certain version. A more recent entry module, e.g. 2022a, is likely to contain more recent versions of the software. module spider will find any version available and using it with name/version will tell you exactly how to load the program. For more information, please consult our Software documentation. File Systems . On our cluster, we have the following storage systems: . | Workernode local storage in /tmp: 2TB of fast SSD storage per node. Great place for active work directories in jobs. | BeeGFS in /beegfs: &gt;900TB shared storage accessible in all nodes. Home directory to all users and intended for storing results and organizing data. I/O intensive jobs should avoid on interacting directly with /beegfs. | CVMFS in /cvmfs: Read-only storage to distribute special software. | NFS in /common/home: Home directory for users of the whep group | . For more information, please consult our file system documentation. ",
    "url": "/gettingstarted/software.html#getting-started-software--file-systems",
    
    "relUrl": "/gettingstarted/software.html#getting-started-software--file-systems"
  },"58": {
    "doc": "Software & File Systems",
    "title": "Software & File Systems",
    "content": " ",
    "url": "/gettingstarted/software.html",
    
    "relUrl": "/gettingstarted/software.html"
  },"59": {
    "doc": "Software on PLEIADES",
    "title": "Software on PLEIADES",
    "content": "Our centrally provided software installations are offered through modules. Some commercial software, e.g. TotalView or ARM Forge, is available as well. Additionally, many groups and users install and use their own software in their respective ${HOME} directories. ",
    "url": "/software.html",
    
    "relUrl": "/software.html"
  },"60": {
    "doc": "2 Factor Authentication on PLEIADES",
    "title": "2 Factor Authentication on PLEIADES",
    "content": " ",
    "url": "/totp.html",
    
    "relUrl": "/totp.html"
  },"61": {
    "doc": "2 Factor Authentication on PLEIADES",
    "title": "Why?",
    "content": "To increase security significantly, two-factor authentication requires a 2nd piece of evidence to log into the PLEIADES cluster. We currently offer TOTP (Time-based one-time password) logins, which usually require no additional hardware on the user’s side - any smartphone is enough. Some people call that “Google Authenticator” because Google implented one reference of this method. But the standard is completely open. Once activated, you need to use an app on your smartphone which will present you a time-limited code, which you have to enter additionally before you will be granted access. Example of such a login (bold text for emphasis of input): . Torsten-Harenbergs-MacBook-Air-2:~ harenber$ ssh fugg2.pleiades.uni-wuppertal.de (harenber@fugg2.pleiades.uni-wuppertal.de) Password: (harenber@fugg2.pleiades.uni-wuppertal.de) Verification code: . Afterwards, you are greeted with the usual splash screen: . Last login: Mon Mar 6 10:29:23 2023 from guckloch.pleiades.uni-wuppertal.de ###### # # # ###### # ## ##### ###### #### # # # # # # # # # # # ###### # ##### # # # # # ##### #### # # # # ###### # # # # # # # # # # # # # # # # ###### ###### # # # ##### ###### #### Cluster documentation: https://pleiadesbuw.github.io/PleiadesUserDocumentation/ General information and tutorials: https://hpc-wiki.info/ Contact in case of questions or problems: pleiades@uni-wuppertal.de NEWS: - 2023-02-28: New GPU dashboard available in our zabbix monitoring service. It provides detailed information about GPU utilization. See our documentation for more information. - 2023-02-27: Maintenance is finished and all nodes are accepting jobs again. - 2022-07-27: In light of the recent security incident: Our Cluster is not affected, but we want to remind you that BeeGFS is not backed up. Make sure to store important results and data on an external device. [harenber@fugg2 ~]$ . ",
    "url": "/totp.html#why",
    
    "relUrl": "/totp.html#why"
  },"62": {
    "doc": "2 Factor Authentication on PLEIADES",
    "title": "How you can use it.",
    "content": "Step 1: Install an App on your smart phone . You will need an app implementing the TOTP standard. There are plenty available, however we would strongly recommend to use a free and open-sourced app. Android . For Android, we recommend FreeOTP+ which is available from Google Play and F-Droid. Once installed, you need to configure FreeOTP+ to protect your TOTPs. Most modern smartphones offer the use of biometrics like using your fingerprint. That makes it really easy to use FreeOTP+. But you can also choose a password to protect the access. iOS . For Apple devices, 2FAS is probably your best choice. Step 2: configure TOTP on PLEIADES . This step is only required once . Log into any PLEIADES login node (whep users: please use higgs or top). Check your quota with beegfs-ctl --getquota --gid $(id -g) (whep users: use quota $(id -u)) and make sure you are not at the limit. Use the google-authenticator program to create a secret and QR code by answering a couple of questions. Shortcut Instead of answering the questions manually, you can execute the command in a short form: . [harenber@fugg2 ~]$ google-authenticator -u -t -D -f -W . The manual process is shown below. Answers are bold and remarks in italics: . [harenber@fugg2 ~]$ google-authenticator Do you want authentication tokens to be time-based (y/n) y Warning: pasting the following URL into your browser exposes the OTP secret to Google: https://www.google.com/chart?` Just ignore the link . Continuing, you will be presented a QR Code, a secret key, a verification code and multiple scratch codes (covered in the FAQ). This is the important part. You will need to scan the QR code with your phone’s TOTP app and you should write down the secret key and codes. Your new secret key is: this is your secret key. Write it down and keep it in a safe place Your verification code is: write this down Your emergency scratch codes are: write them also down and keep them in a safe place Do you want me to update your \"/common/home/harenber/.google_authenticator\" file? (y/n) y Do you want to disallow multiple uses of the same authentication token? This restricts you to one login about every 30s, but it increases your chances to notice or even prevent man-in-the-middle attacks (y/n) n By default, a new token is generated every 30 seconds by the mobile app. In order to compensate for possible time-skew between the client and the server, we allow an extra token before and after the current time. This allows for a time skew of up to 30 seconds between authentication server and client. If you experience problems with poor time synchronization, you can increase the window from its default size of 3 permitted codes (one previous code, the current code, the next code) to 17 permitted codes (the 8 previous codes, the current code, and the 8 next codes). This will permit for a time skew of up to 4 minutes between client and server. Do you want to do so? (y/n) n If the computer that you are logging into isn't hardened against brute-force login attempts, you can enable rate-limiting for the authentication module. By default, this limits attackers to no more than 3 login attempts every 30s. Do you want to enable rate-limiting? (y/n) n . ATTENTION WHEP USERS You have two home directories: the NFS-based /common/home/username on top/higgs and /beegfs/username on the cluster nodes. Please configure TOTP on top/higgs and once you’re finished with the step above, copy the file `.google_authenticator’ to your /beegfs/username directory. Step 3: inserting the secret key into your phone app . This is the easiest step. Use the QR scan method of your app (in FreeOTP+ on the bottom right) and scan the QR code from your terminal. Done. If that does not work, you may add the secret key by hand: . Step 4: enter the verification code now every time you login . When you login to any PLEIADES login node, you will be asked now two questions: . (account@loginnode.pleiades.uni-wuppertal.de) Password: Here you enter your usual password (account@loginnode.pleiades.uni-wuppertal.de) Verification code: Here you enter the time limited code provided by your smart phone app . ",
    "url": "/totp.html#how-you-can-use-it",
    
    "relUrl": "/totp.html#how-you-can-use-it"
  },"63": {
    "doc": "2 Factor Authentication on PLEIADES",
    "title": "FAQ",
    "content": ". | Are passwordless logins impossible with TOTP? . No. If you’re using ssh keys, you can use them as usual. TOTP only affects password-based logins. | What if I loose my phone / get a new phone? . If you’ve written down the secret, you can restore your TOTP on a different device. In addition, FreeOTP+ on Android allows you to export and import your secrets, 2FAS on iOS offers backup/restore in iCloud. In fact, it maskes sense to install a TOTP app on two devices in case you cannot use one. If you fear that you phone is compromized / stolen, you can reset TOTP (see next question). | I think my secret is compromized. You can reset TOTP by deleting .google_authenticator in your home directory and start again from step 2 above. | How to I get rid of this? . Just delete .google_authenticator in your home directory. | TOTP sucks, why don’t you provide FIDO2 keys? . Currently, some of our users still require Red Hat Enterprise Linux 7 which makes it nearly impossible to use FIDO2. | What are these emergency scratch codes that I should write down? . These codes replace a TOTP secret once. So if you cannot use TOTP for whatever reason, each of these codes will replace the TOTP secret. But once you use a code, this one gets invalid (that’s why they are called scratch codes). | HELP! HELP! HELP! I locked myself out! . As usual, your friendly cluster admins are there to help. Just send us a mail to create a ticket. | . ",
    "url": "/totp.html#faq",
    
    "relUrl": "/totp.html#faq"
  },"64": {
    "doc": "Monitoring",
    "title": "Getting Started: Monitoring",
    "content": "Our monitoring service can be accessed at zabbix.pleiades.uni-wuppertal.de with the credentials “pleiades” and “pleiades”. Zabbix collects various information regarding current and past resource usage and quotas. Zabbix is only accessible from the university network! If you are outside and need access, use the ZIMs webvpn. Dashboard . After login you are typically greeted by the user dashboard: . There are also multiple sub-pages available, covering an overview and login-node-specific metrics: . The overview dashboard presents aggregated metrics and you can select the time frame of presented information on the top right. The visible widgets are: . | Number of free (idle) worker or gpu nodes | Current time | SLURM Allocated Cores by Group (2 plots): Current allocation of Cores per group as reported by Slurm | Effective Share used by User Group: Slurm CPU resource usage for each group (resets each month) | Effective Spare of GPU resources by User Group: Slurm GPU resource usage for each group (resets each month) | Beegfs Group Quota (% and TB): Information on how much space is available on BeeGFS per group | . More detailed pages about each class of login nodes provide information about: . | Number of logged in users | CPU usage | Memory usage | . You can change the displayed dashboard through Monitoring &gt; Dashboard &gt; All dashboards &gt; User dashboard: . All available user dashboards are: . GPU Dashboard . Another dashboard shows detailed information of all five gpu nodes, gpu2100[1-5]: . You can select separate pages for each gpu node, which provide detailed information: . These pages can help you answer questions like: . | How much memory is available on a specific GPU? (8 GPUs per node) | How good is the utilization of a specific GPU? | How busy is the node CPU or memory? | How busy is the nodes network interface? | How much power does the GPU consume? | . If you know which GPUs your job is using, or if you use a whole node exclusively, this approach can help to assess your software performance. Details of Specific Hosts . It is possible to show detailed information for specific hosts. Start on the “Monitoring &gt; Hosts” sub page: . Here you can search certain hosts, e.g. a node which is currently involved in processing your Slurm job. You can use squeue -u $USER or scontrol show job &lt;jobid&gt; to get the list of nodes that are processing your job. Keep in mind that nodes are shared between jobs! If you need an exact performance assessment, use the --exclusive flag during job submission to disallow concurrent jobs at the cost of longer waiting times and billing the whole node(s). For each host, you can list all available data, show all predefined diagrams, or a show simple dashboard: . This approach can tell you how well your job utilizes available CPU, memory, InfiniBand, Ethernet, or GPU resources. ",
    "url": "/gettingstarted/zabbix.html#getting-started-monitoring",
    
    "relUrl": "/gettingstarted/zabbix.html#getting-started-monitoring"
  },"65": {
    "doc": "Monitoring",
    "title": "Monitoring",
    "content": " ",
    "url": "/gettingstarted/zabbix.html",
    
    "relUrl": "/gettingstarted/zabbix.html"
  }
}
